{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # data processing\n",
    "import numpy as np # working with arrays\n",
    "import matplotlib.pyplot as plt # visualization\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder , OneHotEncoder\n",
    "from sklearn import preprocessing\n",
    "import math \n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test= {\n",
    "    'KEYVALUES': \"\",               \n",
    "    'RQUID': \"\",                     \n",
    "    'RESULTID': \"\",                 \n",
    "    'ERRORCODE': \"\",                  \n",
    "    'ERRORDESC': \"\",               \n",
    "    'IS_FRAUD': False\n",
    "}\n",
    "prediction_result=1\n",
    "\n",
    "\n",
    "# Case OK\n",
    "if(prediction_result==1):\n",
    "    test['RESULTID']= \"ProceedWithSuccess\"                 \n",
    "    test['ERRORCODE']= \"00000\"              \n",
    "    test['ERRORDESC']= \"PROCESSED_SUCCESSFULLY\"              \n",
    "    test['IS_FRAUD']= True\n",
    "\n",
    "elif(prediction_result==0):\n",
    "    test['RESULTID']= \"ProceedWithSuccessMC\"                 \n",
    "    test['ERRORCODE']= \"99999\"              \n",
    "    test['ERRORDESC']= \"PROCESSED_UNSUCCESSFULLYÂ \"              \n",
    "    test['IS_FRAUD']= False\n",
    "\n",
    "else:\n",
    "    test['RESULTID']= \"SystemError\"                 \n",
    "    test['ERRORCODE']= \"00001\"              \n",
    "    test['ERRORDESC']= \"SYSTEM_ERROR\"  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the credit card transaction data\n",
    "df = pd.read_excel(r'C:\\Users\\KABYADE\\Desktop\\Fraud_ML\\dataset\\clean_data.xlsx',dtype = str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data(data):\n",
    "    list=['V1','V2','V3','V4','V12','V13','V16','V19','V21','V22','V25' ,'V26','V27']\n",
    "    df_back=df.filter(list)\n",
    "\n",
    "    #Check for shape\n",
    "\n",
    "    list=['V6','V7','V23','V8']\n",
    "    #update_dates to d/m/y\n",
    "    #remove_reste \n",
    "    #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_date(data):\n",
    "    #DATETIME\n",
    "    list=['V6','V7','V23','V8']\n",
    "    date_columns=data.filter(list)\n",
    "    year_columns = pd.DataFrame()\n",
    "    \n",
    "    for l in list:\n",
    "        date_columns[l] = pd.to_datetime(date_columns[l])\n",
    "        date_columns[l+'_month'] = date_columns[l].dt.month    \n",
    "        date_columns[l+'_day'] = date_columns[l].dt.day.astype(int)\n",
    "        year_columns[l+'_year'] = date_columns[l].dt.year\n",
    "\n",
    "    date_columns.drop(list, axis=1, inplace=True)\n",
    "    data.drop(list, axis=1, inplace=True)\n",
    "   \n",
    "    #Normalize date cols\n",
    "    min_max_scaler = preprocessing.MinMaxScaler(feature_range=(-math.pi, math.pi))\n",
    "    date_columns_scaled = min_max_scaler.fit_transform(date_columns.values)\n",
    "    date_columns_normalized = pd.DataFrame(date_columns_scaled, columns=date_columns.columns)\n",
    "    date_columns_normalized = date_columns_normalized.applymap(lambda x: math.cos(x))\n",
    "\n",
    "    new_df = pd.concat([data, date_columns_normalized, year_columns], axis=1)\n",
    "    return new_df\n",
    "\n",
    "def update_float(data):\n",
    "    data['V10'] = data['V10'].astype(float)\n",
    "    data['V9'] = data['V9'].astype(float)\n",
    "    data['V24'] = data['V24'].astype(float)\n",
    "    return data\n",
    "\n",
    "def load_dict(col):\n",
    "    with open('json/'+col+'_label_mapping.json', 'r') as f:\n",
    "        label_mapping = json.load(f)\n",
    "    return label_mapping\n",
    "\n",
    "def update_object(data):\n",
    "    list=['V1','V2','V3','V4','V12','V13','V16','V19','V21','V22','V25' ,'V26','V27']\n",
    "    dt=data.filter(list)\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31  ===>  4\n",
      "281  ===>  2\n",
      "22  ===>  23\n",
      "010112  ===>  1\n",
      "010112  ===>  19\n",
      "6011  ===>  2976\n",
      "21140121124C  ===>  44\n",
      "071181  ===>  20339\n",
      "01000002  ===>  3\n",
      "01000002  ===>  1\n",
      "952  ===>  1\n",
      "384  ===>  384\n",
      "VERSUS BANK 2 PLATEAUX REGION LAGUNECI  ===>  4\n"
     ]
    }
   ],
   "source": [
    "data=[  '31', '281', '22',\t'010112',\t'010112', '6011', '21140121124C', '071181',\t\n",
    "        '01000002', '01000002',\t'952',\t'384',\t'VERSUS BANK 2 PLATEAUX REGION LAGUNECI' ]\n",
    "object_cols=['V1','V2','V3','V4','V12','V13','V16','V19','V21','V22','V25' ,'V26','V27']\n",
    "i=0\n",
    "\n",
    "for col in object_cols:\n",
    "    mapping=load_dict(col)\n",
    "    value=data[i]\n",
    "    print(value,\" ===> \",mapping[value])\n",
    "    data[i]=mapping[value]\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_dtype():\n",
    "    df['CLASS'] = df['CLASS'].astype(int)\n",
    "    df['V10'] = df['V10'].astype(float)\n",
    "    df['V9'] = df['V9'].astype(float)\n",
    "    df['V24'] = df['V24'].astype(float)\n",
    "\n",
    "# Object preprocessing \n",
    "def to_categorical(v_df):\n",
    "    cols= v_df.select_dtypes(include=['object'])\n",
    "    obj_cols=cols.columns.to_list()\n",
    "\n",
    "    for col in obj_cols:\n",
    "        le = LabelEncoder()\n",
    "        temp_keys = v_df[col].values\n",
    "        v_df[col] = le.fit_transform(v_df[col])\n",
    "        label_mapping = dict(zip(temp_keys,v_df[col]))\n",
    "        #print(label_mapping)\n",
    "        \n",
    "        with open('json/'+col+'_label_mapping.json', 'w') as f:\n",
    "           json.dump(label_mapping, f)\n",
    "\n",
    "    return v_df\n",
    "\n",
    "def save_mapping(cls,data,col):\n",
    "    label_mapping = dict(zip(cls, data))\n",
    "    #print(label_mapping)\n",
    "    with open('json/'+col+'_label_mapping.json', 'w') as f:\n",
    "        json.dump(label_mapping, f)\n",
    "\n",
    "def process_date():\n",
    "    #DATETIME\n",
    "    list=['V6','V7','V23','V8']\n",
    "    date_columns=df.filter(list)\n",
    "    year_columns = pd.DataFrame()\n",
    "    \n",
    "    for l in list:\n",
    "        date_columns[l] = pd.to_datetime(date_columns[l])\n",
    "        date_columns[l+'_month'] = date_columns[l].dt.month    \n",
    "        date_columns[l+'_day'] = date_columns[l].dt.day.astype(int)\n",
    "        year_columns[l+'_year'] = date_columns[l].dt.year\n",
    "\n",
    "    date_columns.drop(list, axis=1, inplace=True)\n",
    "    df.drop(list, axis=1, inplace=True)\n",
    "   \n",
    "    #Normalize date cols\n",
    "    min_max_scaler = preprocessing.MinMaxScaler(feature_range=(-math.pi, math.pi))\n",
    "    date_columns_scaled = min_max_scaler.fit_transform(date_columns.values)\n",
    "    date_columns_normalized = pd.DataFrame(date_columns_scaled, columns=date_columns.columns)\n",
    "    date_columns_normalized = date_columns_normalized.applymap(lambda x: math.cos(x))\n",
    "\n",
    "    new_df = pd.concat([df, date_columns_normalized, year_columns], axis=1)\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_dtype()\n",
    "df_final=process_date()\n",
    "df_final=to_categorical(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_excel('output/preprocessing_data.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unique value per col\n",
    "for col in df.columns:\n",
    "    check=df[col].unique()\n",
    "    print(str(col)+\" \",len(check),check)\n",
    "\n",
    "df.loc[df['CLASS'] == '0'].V6.describe()\n",
    "# normal '010112', '101010'\n",
    "# fraud '000010', '010112', '101010'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9dce4b41c3dab15c9b172253d5926c6876aa54e726307ae26a963b8959ab689a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
