{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # data processing\n",
    "import numpy as np # working with arrays\n",
    "import matplotlib.pyplot as plt # visualization\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,History\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.metrics import accuracy_score,classification_report,precision_score, recall_score\n",
    "from sklearn.metrics import f1_score, matthews_corrcoef,confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_url):\n",
    "    print(\"Loading data ...\",end=\" \")\n",
    "    df = pd.read_excel(data_url)\n",
    "    #df.drop(['V7_day','V6_day'], axis=1, inplace=True)\n",
    "    X = df.drop(\"CLASS\", axis=1)\n",
    "    y = df[\"CLASS\"]\n",
    "    X = X.values\n",
    "    y = y.values\n",
    "    print('\\033[32m \\u2713 \\033[0m')\n",
    "    return X,y\n",
    "\n",
    "def split_data(X,y):\n",
    "    print(\"Split data ...\",end=\" \")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    print('\\033[32m \\u2713 \\033[0m')\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def save_model(model,name):\n",
    "    print(\"Saving \"+name+\" model ...\",end=\" \")\n",
    "    pickle.dump(model, open(\"./models/\"+name+\".pkl\",\"wb\"))\n",
    "    print('\\033[32m \\u2713 \\033[0m')\n",
    "\n",
    "def print_metrics(y_test,y_pred):\n",
    "    n_errors = (y_pred != y_test).sum()\n",
    "    #print(\"The model used is Decision Tree  classifier\")\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(\"The accuracy is {}\".format(acc))\n",
    "    \n",
    "    prec = precision_score(y_test, y_pred)\n",
    "    print(\"The precision is {}\".format(prec))\n",
    "    \n",
    "    rec = recall_score(y_test, y_pred)\n",
    "    print(\"The recall is {}\".format(rec))\n",
    "    \n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    print(\"The F1-Score is {}\".format(f1))\n",
    "\n",
    "def confusion_matr(y_test,y_pred):\n",
    "    # printing the confusion matrix\n",
    "    LABELS = ['Normal', 'Fraud']\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize =(8, 4))\n",
    "    sns.heatmap(conf_matrix, xticklabels = LABELS,yticklabels = LABELS, annot = True, fmt =\"d\")\n",
    "    plt.title(\"Confusion matrix\")\n",
    "    plt.ylabel('True class')\n",
    "    plt.xlabel('Predicted class')\n",
    "    plt.show()\n",
    "\n",
    "def d_tree(X_train, X_test, y_train, y_test):\n",
    "    print(\"Creating DT model ...\",end=\" \")\n",
    "    clf = DecisionTreeClassifier()\n",
    "    clf.fit(X_train, y_train)\n",
    "    print('\\033[32m \\u2713 \\033[0m')\n",
    "\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print_metrics(y_test,y_pred)\n",
    "    confusion_matr(y_test,y_pred)\n",
    "\n",
    "    return clf\n",
    "\n",
    "def r_forest(X_train, X_test, y_train, y_test):\n",
    "    print(\"Creating RF model ...\",end=\" \")\n",
    "    rfc = RandomForestClassifier()\n",
    "    rfc.fit(X_train, y_train)\n",
    "    print('\\033[32m \\u2713 \\033[0m')\n",
    "\n",
    "    y_pred = rfc.predict(X_test)\n",
    "    print_metrics(y_test,y_pred)\n",
    "    confusion_matr(y_test,y_pred)\n",
    "    \n",
    "    return rfc\n",
    "\n",
    "def ex_tree(X_train, X_test, y_train, y_test):\n",
    "    print(\"Creating Extra_T model ...\",end=\" \")\n",
    "    etc = ExtraTreesClassifier(n_estimators=100, max_depth=4)\n",
    "    etc.fit(X_train, y_train)\n",
    "    print('\\033[32m \\u2713 \\033[0m')\n",
    "    y_pred = etc.predict(X_test)\n",
    "    print_metrics(y_test,y_pred)\n",
    "    confusion_matr(y_test,y_pred)\n",
    "    return etc\n",
    "\n",
    "num_classes=63\n",
    "\n",
    "def cnn_md(X,y):\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = split_data(X,y)\n",
    "    \n",
    "    print(\"Creating CNN model ...\",end=\" \")\n",
    "\n",
    "    # Define the CNN model\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Reshape((X.shape[1], 1), input_shape=(X.shape[1],)  ))\n",
    "    model.add(tf.keras.layers.Conv1D(32, 2, activation='relu'))\n",
    "    model.add(tf.keras.layers.MaxPooling1D(2))\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    \n",
    "    print('\\033[32m \\u2713 \\033[0m')  \n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    history= History()\n",
    "    \n",
    "    print(\"Training step ...\")\n",
    "    # Train the model\n",
    "    history= model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))\n",
    "    print('\\033[32m \\u2713 \\033[0m')  \n",
    "\n",
    "    # Evaluate the model on the test data\n",
    "    test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "    print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "    # Make predictions on the test data\n",
    "    # y_pred = model.predict(X_test)\n",
    "    # confusion_matr(y_test,y_pred) \n",
    "\n",
    "    return history,model\n",
    "\n",
    "def ensemble_learning(X_train, X_test, y_train, y_test,mds):\n",
    "    dt,df=mds\n",
    "    # Combine the models into an ensemble\n",
    "    ensemble_clf = VotingClassifier(estimators=[(\"dt\", dt), (\"rf\", rd)], voting='hard')\n",
    "    ensemble_clf.fit(X_train, y_train)\n",
    "    y_pred = ensemble_clf.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_url):\n",
    "    print(\"Loading data 2 ...\",end=\" \")\n",
    "    df = pd.read_excel(data_url)\n",
    "    #df.drop(['V7_day','V6_day'], axis=1, inplace=True)\n",
    "    X = df.drop(\"CLASS\", axis=1)\n",
    "    y = df[\"CLASS\"]\n",
    "    print('\\033[32m \\u2713 \\033[0m')\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data 2 ... \u001b[32m âœ“ \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "data_url='C:/Users/KABYADE/Desktop/Fraud_ML/dataset/preprocessing_data.xlsx'\n",
    "X,y=load_data(data_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i  in range(len(y)):\n",
    "    y[i]=y[i]-1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = X.values # list of sequences\n",
    "y = tf.keras.utils.to_categorical(y,len(np.unique(y)))\n",
    "labels = y # list of class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating CNN model ... Model: \"sequential_40\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_38 (Conv1D)          (None, 34, 32)            128       \n",
      "                                                                 \n",
      " max_pooling1d_37 (MaxPoolin  (None, 17, 32)           0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " flatten_32 (Flatten)        (None, 544)               0         \n",
      "                                                                 \n",
      " dense_85 (Dense)            (None, 64)                34880     \n",
      "                                                                 \n",
      " dense_86 (Dense)            (None, 63)                4095      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 39,103\n",
      "Trainable params: 39,103\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating CNN model ...\",end=\" \")\n",
    "# Define the CNN model\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Conv1D(32, 3, input_shape = (36,1), activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling1D(2))\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(63, activation='softmax'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history= History()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training step ...\n",
      "Epoch 1/30\n",
      "2500/2500 [==============================] - 13s 5ms/step - loss: 1.8903 - accuracy: 0.4415 - val_loss: 1.3378 - val_accuracy: 0.5612\n",
      "Epoch 2/30\n",
      "2500/2500 [==============================] - 16s 6ms/step - loss: 1.2138 - accuracy: 0.5759 - val_loss: 1.1453 - val_accuracy: 0.5755\n",
      "Epoch 3/30\n",
      "2500/2500 [==============================] - 16s 6ms/step - loss: 1.0815 - accuracy: 0.6047 - val_loss: 1.0577 - val_accuracy: 0.6120\n",
      "Epoch 4/30\n",
      "2500/2500 [==============================] - 18s 7ms/step - loss: 1.0109 - accuracy: 0.6290 - val_loss: 1.0405 - val_accuracy: 0.6081\n",
      "Epoch 5/30\n",
      "2500/2500 [==============================] - 18s 7ms/step - loss: 0.9561 - accuracy: 0.6461 - val_loss: 0.9508 - val_accuracy: 0.6668\n",
      "Epoch 6/30\n",
      "2500/2500 [==============================] - 18s 7ms/step - loss: 0.9065 - accuracy: 0.6629 - val_loss: 0.9078 - val_accuracy: 0.6611\n",
      "Epoch 7/30\n",
      "2500/2500 [==============================] - 18s 7ms/step - loss: 0.8653 - accuracy: 0.6749 - val_loss: 0.8408 - val_accuracy: 0.6909\n",
      "Epoch 8/30\n",
      "2500/2500 [==============================] - 18s 7ms/step - loss: 0.8344 - accuracy: 0.6829 - val_loss: 0.8394 - val_accuracy: 0.6856\n",
      "Epoch 9/30\n",
      "2500/2500 [==============================] - 19s 8ms/step - loss: 0.8078 - accuracy: 0.6909 - val_loss: 0.8284 - val_accuracy: 0.6819\n",
      "Epoch 10/30\n",
      "2500/2500 [==============================] - 20s 8ms/step - loss: 0.7893 - accuracy: 0.6961 - val_loss: 0.8009 - val_accuracy: 0.6937\n",
      "Epoch 11/30\n",
      "2500/2500 [==============================] - 18s 7ms/step - loss: 0.7689 - accuracy: 0.7008 - val_loss: 0.7768 - val_accuracy: 0.7043\n",
      "Epoch 12/30\n",
      "2500/2500 [==============================] - 18s 7ms/step - loss: 0.7536 - accuracy: 0.7028 - val_loss: 0.7755 - val_accuracy: 0.6982\n",
      "Epoch 13/30\n",
      "2500/2500 [==============================] - 19s 7ms/step - loss: 0.7426 - accuracy: 0.7054 - val_loss: 0.7699 - val_accuracy: 0.7002\n",
      "Epoch 14/30\n",
      "2500/2500 [==============================] - 18s 7ms/step - loss: 0.7298 - accuracy: 0.7077 - val_loss: 0.7475 - val_accuracy: 0.7048\n",
      "Epoch 15/30\n",
      "2500/2500 [==============================] - 14s 5ms/step - loss: 0.7248 - accuracy: 0.7092 - val_loss: 0.7319 - val_accuracy: 0.7050\n",
      "Epoch 16/30\n",
      "2500/2500 [==============================] - 16s 6ms/step - loss: 0.7125 - accuracy: 0.7133 - val_loss: 0.7531 - val_accuracy: 0.6954\n",
      "Epoch 17/30\n",
      "2500/2500 [==============================] - 15s 6ms/step - loss: 0.7054 - accuracy: 0.7142 - val_loss: 0.7367 - val_accuracy: 0.7012\n",
      "Epoch 18/30\n",
      "2500/2500 [==============================] - 15s 6ms/step - loss: 0.6999 - accuracy: 0.7152 - val_loss: 0.7246 - val_accuracy: 0.7066\n",
      "Epoch 19/30\n",
      "2500/2500 [==============================] - 15s 6ms/step - loss: 0.6921 - accuracy: 0.7176 - val_loss: 0.7121 - val_accuracy: 0.7177\n",
      "Epoch 20/30\n",
      "2500/2500 [==============================] - 17s 7ms/step - loss: 0.6852 - accuracy: 0.7187 - val_loss: 0.6972 - val_accuracy: 0.7171\n",
      "Epoch 21/30\n",
      "2500/2500 [==============================] - 16s 6ms/step - loss: 0.6833 - accuracy: 0.7205 - val_loss: 0.7126 - val_accuracy: 0.7059\n",
      "Epoch 22/30\n",
      "2500/2500 [==============================] - 16s 6ms/step - loss: 0.6746 - accuracy: 0.7221 - val_loss: 0.6906 - val_accuracy: 0.7116\n",
      "Epoch 23/30\n",
      "2500/2500 [==============================] - 15s 6ms/step - loss: 0.6716 - accuracy: 0.7223 - val_loss: 0.7005 - val_accuracy: 0.7158\n",
      "Epoch 24/30\n",
      "2500/2500 [==============================] - 15s 6ms/step - loss: 0.6670 - accuracy: 0.7237 - val_loss: 0.6952 - val_accuracy: 0.7161\n",
      "Epoch 25/30\n",
      "2500/2500 [==============================] - 14s 6ms/step - loss: 0.6643 - accuracy: 0.7241 - val_loss: 0.7125 - val_accuracy: 0.7134\n",
      "Epoch 26/30\n",
      "2500/2500 [==============================] - 14s 6ms/step - loss: 0.6584 - accuracy: 0.7266 - val_loss: 0.6843 - val_accuracy: 0.7183\n",
      "Epoch 27/30\n",
      "2500/2500 [==============================] - 14s 6ms/step - loss: 0.6534 - accuracy: 0.7270 - val_loss: 0.7002 - val_accuracy: 0.7132\n",
      "Epoch 28/30\n",
      "2500/2500 [==============================] - 14s 6ms/step - loss: 0.6528 - accuracy: 0.7284 - val_loss: 0.6817 - val_accuracy: 0.7233\n",
      "Epoch 29/30\n",
      "2500/2500 [==============================] - 14s 6ms/step - loss: 0.6475 - accuracy: 0.7325 - val_loss: 0.6924 - val_accuracy: 0.7067\n",
      "Epoch 30/30\n",
      "2500/2500 [==============================] - 16s 6ms/step - loss: 0.6456 - accuracy: 0.7318 - val_loss: 0.7040 - val_accuracy: 0.7168\n",
      "\u001b[32m âœ“ \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print(\"Training step ...\")\n",
    "# Train the model\n",
    "history= model.fit(X_train, y_train, epochs=30, batch_size=32, validation_data=(X_test, y_test))\n",
    "print('\\033[32m \\u2713 \\033[0m') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "\n",
    "X_train, X_test, y_train, y_test = split_data(X,y)\n",
    "print(X_train.shape)\n",
    "\n",
    "data=X\n",
    "labels=y\n",
    "\n",
    "# create a time series generator\n",
    "generator = TimeseriesGenerator(data, labels,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn=cnn_md(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating DT model ... \u001b[32m âœ“ \u001b[0m\n",
      "The accuracy is 0.7665\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted'].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m#Decision_Tree\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m dt\u001b[39m=\u001b[39md_tree(X_train, X_test, y_train, y_test)\n",
      "Cell \u001b[1;32mIn[30], line 57\u001b[0m, in \u001b[0;36md_tree\u001b[1;34m(X_train, X_test, y_train, y_test)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m\\033\u001b[39;00m\u001b[39m[32m \u001b[39m\u001b[39m\\u2713\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\033\u001b[39;00m\u001b[39m[0m\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     56\u001b[0m y_pred \u001b[39m=\u001b[39m clf\u001b[39m.\u001b[39mpredict(X_test)\n\u001b[1;32m---> 57\u001b[0m print_metrics(y_test,y_pred)\n\u001b[0;32m     58\u001b[0m confusion_matr(y_test,y_pred)\n\u001b[0;32m     60\u001b[0m \u001b[39mreturn\u001b[39;00m clf\n",
      "Cell \u001b[1;32mIn[30], line 30\u001b[0m, in \u001b[0;36mprint_metrics\u001b[1;34m(y_test, y_pred)\u001b[0m\n\u001b[0;32m     27\u001b[0m acc \u001b[39m=\u001b[39m accuracy_score(y_test, y_pred)\n\u001b[0;32m     28\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mThe accuracy is \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(acc))\n\u001b[1;32m---> 30\u001b[0m prec \u001b[39m=\u001b[39m precision_score(y_test, y_pred)\n\u001b[0;32m     31\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mThe precision is \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(prec))\n\u001b[0;32m     33\u001b[0m rec \u001b[39m=\u001b[39m recall_score(y_test, y_pred)\n",
      "File \u001b[1;32mc:\\Users\\KABYADE\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1954\u001b[0m, in \u001b[0;36mprecision_score\u001b[1;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1825\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprecision_score\u001b[39m(\n\u001b[0;32m   1826\u001b[0m     y_true,\n\u001b[0;32m   1827\u001b[0m     y_pred,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1833\u001b[0m     zero_division\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mwarn\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1834\u001b[0m ):\n\u001b[0;32m   1835\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Compute the precision.\u001b[39;00m\n\u001b[0;32m   1836\u001b[0m \n\u001b[0;32m   1837\u001b[0m \u001b[39m    The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1952\u001b[0m \u001b[39m    array([0.5, 1. , 1. ])\u001b[39;00m\n\u001b[0;32m   1953\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1954\u001b[0m     p, _, _, _ \u001b[39m=\u001b[39m precision_recall_fscore_support(\n\u001b[0;32m   1955\u001b[0m         y_true,\n\u001b[0;32m   1956\u001b[0m         y_pred,\n\u001b[0;32m   1957\u001b[0m         labels\u001b[39m=\u001b[39;49mlabels,\n\u001b[0;32m   1958\u001b[0m         pos_label\u001b[39m=\u001b[39;49mpos_label,\n\u001b[0;32m   1959\u001b[0m         average\u001b[39m=\u001b[39;49maverage,\n\u001b[0;32m   1960\u001b[0m         warn_for\u001b[39m=\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39mprecision\u001b[39;49m\u001b[39m\"\u001b[39;49m,),\n\u001b[0;32m   1961\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m   1962\u001b[0m         zero_division\u001b[39m=\u001b[39;49mzero_division,\n\u001b[0;32m   1963\u001b[0m     )\n\u001b[0;32m   1964\u001b[0m     \u001b[39mreturn\u001b[39;00m p\n",
      "File \u001b[1;32mc:\\Users\\KABYADE\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1573\u001b[0m, in \u001b[0;36mprecision_recall_fscore_support\u001b[1;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1571\u001b[0m \u001b[39mif\u001b[39;00m beta \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1572\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mbeta should be >=0 in the F-beta score\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 1573\u001b[0m labels \u001b[39m=\u001b[39m _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)\n\u001b[0;32m   1575\u001b[0m \u001b[39m# Calculate tp_sum, pred_sum, true_sum ###\u001b[39;00m\n\u001b[0;32m   1576\u001b[0m samplewise \u001b[39m=\u001b[39m average \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msamples\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\KABYADE\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1391\u001b[0m, in \u001b[0;36m_check_set_wise_labels\u001b[1;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[0;32m   1389\u001b[0m         \u001b[39mif\u001b[39;00m y_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmulticlass\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m   1390\u001b[0m             average_options\u001b[39m.\u001b[39mremove(\u001b[39m\"\u001b[39m\u001b[39msamples\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 1391\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1392\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mTarget is \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m but average=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbinary\u001b[39m\u001b[39m'\u001b[39m\u001b[39m. Please \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1393\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mchoose another average setting, one of \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (y_type, average_options)\n\u001b[0;32m   1394\u001b[0m         )\n\u001b[0;32m   1395\u001b[0m \u001b[39melif\u001b[39;00m pos_label \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m (\u001b[39mNone\u001b[39;00m, \u001b[39m1\u001b[39m):\n\u001b[0;32m   1396\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   1397\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNote that pos_label (set to \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m) is ignored when \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1398\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39maverage != \u001b[39m\u001b[39m'\u001b[39m\u001b[39mbinary\u001b[39m\u001b[39m'\u001b[39m\u001b[39m (got \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m). You may use \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1401\u001b[0m         \u001b[39mUserWarning\u001b[39;00m,\n\u001b[0;32m   1402\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted']."
     ]
    }
   ],
   "source": [
    "#Decision_Tree\n",
    "dt=d_tree(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating RF model ... \u001b[32m âœ“ \u001b[0m\n",
      "The accuracy is 0.7601\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted'].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m rd\u001b[39m=\u001b[39mr_forest(X_train, X_test, y_train, y_test)\n",
      "Cell \u001b[1;32mIn[30], line 69\u001b[0m, in \u001b[0;36mr_forest\u001b[1;34m(X_train, X_test, y_train, y_test)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m\\033\u001b[39;00m\u001b[39m[32m \u001b[39m\u001b[39m\\u2713\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\033\u001b[39;00m\u001b[39m[0m\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     68\u001b[0m y_pred \u001b[39m=\u001b[39m rfc\u001b[39m.\u001b[39mpredict(X_test)\n\u001b[1;32m---> 69\u001b[0m print_metrics(y_test,y_pred)\n\u001b[0;32m     70\u001b[0m confusion_matr(y_test,y_pred)\n\u001b[0;32m     72\u001b[0m \u001b[39mreturn\u001b[39;00m rfc\n",
      "Cell \u001b[1;32mIn[30], line 30\u001b[0m, in \u001b[0;36mprint_metrics\u001b[1;34m(y_test, y_pred)\u001b[0m\n\u001b[0;32m     27\u001b[0m acc \u001b[39m=\u001b[39m accuracy_score(y_test, y_pred)\n\u001b[0;32m     28\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mThe accuracy is \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(acc))\n\u001b[1;32m---> 30\u001b[0m prec \u001b[39m=\u001b[39m precision_score(y_test, y_pred)\n\u001b[0;32m     31\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mThe precision is \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(prec))\n\u001b[0;32m     33\u001b[0m rec \u001b[39m=\u001b[39m recall_score(y_test, y_pred)\n",
      "File \u001b[1;32mc:\\Users\\KABYADE\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1954\u001b[0m, in \u001b[0;36mprecision_score\u001b[1;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1825\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprecision_score\u001b[39m(\n\u001b[0;32m   1826\u001b[0m     y_true,\n\u001b[0;32m   1827\u001b[0m     y_pred,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1833\u001b[0m     zero_division\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mwarn\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1834\u001b[0m ):\n\u001b[0;32m   1835\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Compute the precision.\u001b[39;00m\n\u001b[0;32m   1836\u001b[0m \n\u001b[0;32m   1837\u001b[0m \u001b[39m    The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1952\u001b[0m \u001b[39m    array([0.5, 1. , 1. ])\u001b[39;00m\n\u001b[0;32m   1953\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1954\u001b[0m     p, _, _, _ \u001b[39m=\u001b[39m precision_recall_fscore_support(\n\u001b[0;32m   1955\u001b[0m         y_true,\n\u001b[0;32m   1956\u001b[0m         y_pred,\n\u001b[0;32m   1957\u001b[0m         labels\u001b[39m=\u001b[39;49mlabels,\n\u001b[0;32m   1958\u001b[0m         pos_label\u001b[39m=\u001b[39;49mpos_label,\n\u001b[0;32m   1959\u001b[0m         average\u001b[39m=\u001b[39;49maverage,\n\u001b[0;32m   1960\u001b[0m         warn_for\u001b[39m=\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39mprecision\u001b[39;49m\u001b[39m\"\u001b[39;49m,),\n\u001b[0;32m   1961\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m   1962\u001b[0m         zero_division\u001b[39m=\u001b[39;49mzero_division,\n\u001b[0;32m   1963\u001b[0m     )\n\u001b[0;32m   1964\u001b[0m     \u001b[39mreturn\u001b[39;00m p\n",
      "File \u001b[1;32mc:\\Users\\KABYADE\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1573\u001b[0m, in \u001b[0;36mprecision_recall_fscore_support\u001b[1;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1571\u001b[0m \u001b[39mif\u001b[39;00m beta \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1572\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mbeta should be >=0 in the F-beta score\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 1573\u001b[0m labels \u001b[39m=\u001b[39m _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)\n\u001b[0;32m   1575\u001b[0m \u001b[39m# Calculate tp_sum, pred_sum, true_sum ###\u001b[39;00m\n\u001b[0;32m   1576\u001b[0m samplewise \u001b[39m=\u001b[39m average \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msamples\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\KABYADE\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1391\u001b[0m, in \u001b[0;36m_check_set_wise_labels\u001b[1;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[0;32m   1389\u001b[0m         \u001b[39mif\u001b[39;00m y_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmulticlass\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m   1390\u001b[0m             average_options\u001b[39m.\u001b[39mremove(\u001b[39m\"\u001b[39m\u001b[39msamples\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 1391\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1392\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mTarget is \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m but average=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbinary\u001b[39m\u001b[39m'\u001b[39m\u001b[39m. Please \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1393\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mchoose another average setting, one of \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (y_type, average_options)\n\u001b[0;32m   1394\u001b[0m         )\n\u001b[0;32m   1395\u001b[0m \u001b[39melif\u001b[39;00m pos_label \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m (\u001b[39mNone\u001b[39;00m, \u001b[39m1\u001b[39m):\n\u001b[0;32m   1396\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   1397\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNote that pos_label (set to \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m) is ignored when \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1398\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39maverage != \u001b[39m\u001b[39m'\u001b[39m\u001b[39mbinary\u001b[39m\u001b[39m'\u001b[39m\u001b[39m (got \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m). You may use \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1401\u001b[0m         \u001b[39mUserWarning\u001b[39;00m,\n\u001b[0;32m   1402\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted']."
     ]
    }
   ],
   "source": [
    "rd=r_forest(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(rd,\"R_forest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data=[  4,  2,  23,  1,  410878,  715848.0,  19,  2976,  44,  20339,  3,  1,  621524,  1,  \n",
    "             384,  4,  0.654861,  0.654861, -1.0, -0.5,  0.959493, -0.978148,  2009,  2011,  2009,  2009 \n",
    "           ]\n",
    "to_predict = np.array(test_data).reshape(1,26)\n",
    "\n",
    "rf_model = pickle.load(open(\"models/R_forest.pkl\",\"rb\"))\n",
    "rf_model.predict(to_predict) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "38db7db662a93b92d35804601f8e71aa4f29fb454ddd5b4d70af8eaaf7f2bce7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
